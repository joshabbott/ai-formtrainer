{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07f4b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a63fbbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "# import PyQt5\n",
    "from PIL import Image\n",
    "from IPython.display import Video\n",
    "import my_nb_helpers\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "poselandmarks_list = my_nb_helpers.poselandmarks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d647e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = './videos-squats/squats-front-jim.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec65f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = './videos-squats/output/output_squats-front-jim.avi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab4bead0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video was successfully saved\n"
     ]
    }
   ],
   "source": [
    "with mp_pose.Pose(min_detection_confidence=0.5,min_tracking_confidence=0.5) as pose:\n",
    "    # Create VideoCapture object\n",
    "    cap = cv2.VideoCapture(input_filename)\n",
    "\n",
    "\t# Raise error if file cannot be opened\n",
    "    if cap.isOpened() == False:\n",
    "        print(\"Error opening video stream or file\")\n",
    "        raise TypeError\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    size = (frame_width, frame_height)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    # Get the number of frames in the video\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # create videowriter object to create new video with pose estimation overlayed\n",
    "    videowriter = cv2.VideoWriter(output_filename,cv2.VideoWriter_fourcc(*'MJPG'),fps,size)\n",
    "    \n",
    "    # Create a NumPy array to store the pose data as before\n",
    "    # The shape is 3x33x144 - 3D XYZ data for 33 landmarks across 144 frames\n",
    "    # data = np.empty((3, len(poselandmarks_list), length))    \n",
    "    data = np.empty((3, 33, length))    \n",
    "    \n",
    "\t# For each image in the video, extract the spatial pose data and save it in the appropriate spot in the `data` array \n",
    "    frame_num = 0\n",
    "    \n",
    "    \n",
    "    while cap.isOpened():\n",
    "        # read current frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # recolor frame to RGB and save to new cv2 object'image'\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # make detection\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        # recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # get landmark results for current frame\n",
    "        landmarks = results.pose_world_landmarks.landmark\n",
    "\n",
    "        # fill in coordinate data for each landmark for current frame\n",
    "        for i in range(len(mp_pose.PoseLandmark)):\n",
    "            data[:, i, frame_num] = (landmarks[i].x, landmarks[i].y, landmarks[i].z)  \n",
    "        \n",
    "        frame_num += 1\n",
    "        \n",
    "        # render detection\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks,mp_pose.POSE_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(245,117,66),thickness=2, circle_radius=2),\n",
    "                                  mp_drawing.DrawingSpec(color=(245,66,230),thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "        # write pose estimation overlay video data\n",
    "        videowriter.write(image)\n",
    "        # display image (not sure we need to do this)\n",
    "        cv2.imshow(\"Mediapipe Frame\", image)\n",
    "    \n",
    "    cap.release()\n",
    "    videowriter.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"The video was successfully saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43db6821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable             Type                          Data/Info\n",
      "------------------------------------------------------------\n",
      "Image                module                        <module 'PIL.Image' from <...>e-packages/PIL/Image.py'>\n",
      "Video                type                          <class 'IPython.core.display.Video'>\n",
      "anim                 FuncAnimation                 <matplotlib.animation.Fun<...>object at 0x7fc2a42536d0>\n",
      "animation            module                        <module 'matplotlib.anima<...>matplotlib/animation.py'>\n",
      "ax                   Axes3DSubplot                 Axes3DSubplot(0.1275,0.11;0.77x0.77)\n",
      "cap                  VideoCapture                  < cv2.VideoCapture 0x7fc2c339d1b0>\n",
      "cv2                  module                        <module 'cv2' from '/User<...>ackages/cv2/__init__.py'>\n",
      "data                 ndarray                       3x33x498: 49302 elems, type `float64`, 394416 bytes (385.171875 kb)\n",
      "fig                  Figure                        Figure(1400x1400)\n",
      "file                 str                           ./videos-squats/squats-front-jim.mp4\n",
      "fps                  int                           29\n",
      "frame                NoneType                      None\n",
      "frame_height         int                           540\n",
      "frame_num            int                           498\n",
      "frame_width          int                           960\n",
      "i                    int                           32\n",
      "image                ndarray                       540x960x3: 1555200 elems, type `uint8`, 1555200 bytes (1.483154296875 Mb)\n",
      "input_filename       str                           ./videos-squats/squats-front-jim.mp4\n",
      "landmarks            RepeatedCompositeContainer    [x: 0.029324736446142197\\<...>ty: 0.9951229095458984\\n]\n",
      "length               int                           498\n",
      "mp                   module                        <module 'mediapipe' from <...>s/mediapipe/__init__.py'>\n",
      "mp_drawing           module                        <module 'mediapipe.python<...>utions/drawing_utils.py'>\n",
      "mp_drawing_styles    module                        <module 'mediapipe.python<...>tions/drawing_styles.py'>\n",
      "mp_face_mesh         module                        <module 'mediapipe.python<...>/solutions/face_mesh.py'>\n",
      "mp_holistic          module                        <module 'mediapipe.python<...>n/solutions/holistic.py'>\n",
      "mp_pose              module                        <module 'mediapipe.python<...>ython/solutions/pose.py'>\n",
      "mpl                  module                        <module 'matplotlib' from<...>/matplotlib/__init__.py'>\n",
      "my_nb_helpers        module                        <module 'my_nb_helpers' f<...>o_code/my_nb_helpers.py'>\n",
      "np                   module                        <module 'numpy' from '/Us<...>kages/numpy/__init__.py'>\n",
      "output_filename      str                           ./output_squats-front-jim.avi\n",
      "pickle               module                        <module 'pickle' from '/o<...>lib/python3.7/pickle.py'>\n",
      "plt                  module                        <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n",
      "pose                 Pose                          <mediapipe.python.solutio<...>object at 0x7fc2a3a6b850>\n",
      "poselandmarks_list   list                          n=33\n",
      "result               VideoWriter                   < cv2.VideoWriter 0x7fc2c338bf90>\n",
      "results              type                          <class 'mediapipe.python.<...>on_base.SolutionOutputs'>\n",
      "ret                  bool                          False\n",
      "size                 tuple                         n=2\n",
      "urllib               module                        <module 'urllib' from '/o<...>n3.7/urllib/__init__.py'>\n",
      "videowriter          VideoWriter                   < cv2.VideoWriter 0x7fc2c33f2990>\n"
     ]
    }
   ],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1d7c0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PoseLandmark.NOSE: 0>,\n",
       " <PoseLandmark.LEFT_EYE_INNER: 1>,\n",
       " <PoseLandmark.LEFT_EYE: 2>,\n",
       " <PoseLandmark.LEFT_EYE_OUTER: 3>,\n",
       " <PoseLandmark.RIGHT_EYE_INNER: 4>,\n",
       " <PoseLandmark.RIGHT_EYE: 5>,\n",
       " <PoseLandmark.RIGHT_EYE_OUTER: 6>,\n",
       " <PoseLandmark.LEFT_EAR: 7>,\n",
       " <PoseLandmark.RIGHT_EAR: 8>,\n",
       " <PoseLandmark.MOUTH_LEFT: 9>,\n",
       " <PoseLandmark.MOUTH_RIGHT: 10>,\n",
       " <PoseLandmark.LEFT_SHOULDER: 11>,\n",
       " <PoseLandmark.RIGHT_SHOULDER: 12>,\n",
       " <PoseLandmark.LEFT_ELBOW: 13>,\n",
       " <PoseLandmark.RIGHT_ELBOW: 14>,\n",
       " <PoseLandmark.LEFT_WRIST: 15>,\n",
       " <PoseLandmark.RIGHT_WRIST: 16>,\n",
       " <PoseLandmark.LEFT_PINKY: 17>,\n",
       " <PoseLandmark.RIGHT_PINKY: 18>,\n",
       " <PoseLandmark.LEFT_INDEX: 19>,\n",
       " <PoseLandmark.RIGHT_INDEX: 20>,\n",
       " <PoseLandmark.LEFT_THUMB: 21>,\n",
       " <PoseLandmark.RIGHT_THUMB: 22>,\n",
       " <PoseLandmark.LEFT_HIP: 23>,\n",
       " <PoseLandmark.RIGHT_HIP: 24>,\n",
       " <PoseLandmark.LEFT_KNEE: 25>,\n",
       " <PoseLandmark.RIGHT_KNEE: 26>,\n",
       " <PoseLandmark.LEFT_ANKLE: 27>,\n",
       " <PoseLandmark.RIGHT_ANKLE: 28>,\n",
       " <PoseLandmark.LEFT_HEEL: 29>,\n",
       " <PoseLandmark.RIGHT_HEEL: 30>,\n",
       " <PoseLandmark.LEFT_FOOT_INDEX: 31>,\n",
       " <PoseLandmark.RIGHT_FOOT_INDEX: 32>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can go through the data now and compute angles for a few landmarks of interest\n",
    "\n",
    "# WILL NEED TO ASSUME CAMERA FRONT OR SIDE (START WITH FROM LEFT SIDE VIEW)\n",
    "\n",
    "\n",
    "# interesting landmarks\n",
    "list(mp_pose.PoseLandmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ace6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce3cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf9e266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e74811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "\n",
    "\t# Create VideoCapture object\n",
    "    cap = cv2.VideoCapture(file)\n",
    "\n",
    "\t# Raise error if file cannot be opened\n",
    "    if cap.isOpened() == False:\n",
    "        print(\"Error opening video stream or file\")\n",
    "        raise TypeError\n",
    "\n",
    "\t# Get the number of frames in the video\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Create a NumPy array to store the pose data as before\n",
    "    # The shape is 3x33x144 - 3D XYZ data for 33 landmarks across 144 frames\n",
    "    # data = np.empty((3, len(poselandmarks_list), length))    \n",
    "    data = np.empty((3, 33, length))    \n",
    "    \n",
    "\t# For each image in the video, extract the spatial pose data and save it in the appropriate spot in the `data` array \n",
    "    frame_num = 0\n",
    "    while cap.isOpened():\n",
    "        ret, image = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        landmarks = results.pose_world_landmarks.landmark\n",
    "        # landmarks = results.pose_landmarks.landmark\n",
    "        for i in range(len(mp_pose.PoseLandmark)):\n",
    "            data[:, i, frame_num] = (landmarks[i].x, landmarks[i].y, landmarks[i].z)  \n",
    "        \n",
    "        frame_num += 1\n",
    "    \n",
    "    # Close the video file\n",
    "    cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "211cdbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "446"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e05cc81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# fig.set_size_inches(7, 7, True)\n",
    "# ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "# anim = my_nb_helpers.time_animate(data, fig, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bf8bd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2c6b35f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.animation.FuncAnimation at 0x7fdd99531390>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3cde4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
